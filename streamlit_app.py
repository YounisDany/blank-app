import os
import json
import streamlit as st
import requests
from bs4 import BeautifulSoup
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.chains import ConversationalRetrievalChain
from langchain.schema import Document
from langchain.llms import OpenAI

# ØªØ¹ÙŠÙŠÙ† Ù…ÙØªØ§Ø­ OpenAI API ÙƒÙ…ØªØ­ÙˆÙ„ Ø¨ÙŠØ¦ÙŠ
os.environ["OPENAI_API_KEY"] = "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"  # Ø§Ø³ØªØ¨Ø¯Ù„ 'YOUR_OPENAI_API_KEY' Ø¨Ø§Ù„Ù…ÙØªØ§Ø­ Ø§Ù„ÙØ¹Ù„ÙŠ

# ØªØ­Ù…ÙŠÙ„ Ù…Ø­ØªÙˆÙ‰ ØµÙØ­Ø© ÙˆÙŠØ¨ Ù…Ù† Ø±Ø§Ø¨Ø· HTML
def load_web_content(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    paragraphs = soup.find_all('p')
    text = ' '.join([p.get_text() for p in paragraphs])
    return text

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ù…Ù„Ù links.json ÙˆØªØ­Ù„ÙŠÙ„Ù‡Ø§ ÙƒØµÙØ­Ø§Øª HTML
def load_links():
    links_file_path = os.path.join("data", "links.json")
    documents = []

    if os.path.exists(links_file_path):
        try:
            with open(links_file_path, "r", encoding="utf-8") as file:
                data = json.load(file)
                for link in data["links"]:
                    content = load_web_content(link)
                    documents.append(Document(page_content=content, metadata={"source": link}))
        except json.JSONDecodeError:
            st.error("Ù‡Ù†Ø§Ùƒ Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø³ÙŠÙ‚ Ù…Ù„Ù links.json. ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ù„Ù ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ JSON ØµØ§Ù„Ø­.")
    
    return documents

# Ø¥Ø¹Ø¯Ø§Ø¯ LangChain Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
def setup_chain(documents):
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    docs = text_splitter.split_documents(documents)
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø®Ø·Ø§Ø¡ ØªÙ‡ÙŠØ¦Ø© OpenAIEmbeddings
    try:
        embeddings = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))
    except Exception as e:
        st.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªÙ‡ÙŠØ¦Ø© OpenAIEmbeddings: {e}")
        return None
    
    db = FAISS.from_documents(docs, embeddings)
    return ConversationalRetrievalChain(OpenAI(), db.as_retriever())

# ÙˆØ§Ø¬Ù‡Ø© Streamlit Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…
st.set_page_config(page_title="Chatbot - Ask Only", page_icon="ğŸ¤–")
st.title("Chatbot ğŸ¤– - Ø§Ø³Ø£Ù„Ù†ÙŠ ÙÙ‚Ø· Ù…Ù† Ø§Ù„Ø±ÙˆØ§Ø¨Ø·")
st.write("ÙŠÙ…ÙƒÙ†Ùƒ Ø·Ø±Ø­ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙŠ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ù…Ù„Ù links.json.")

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
documents = load_links()
chain = setup_chain(documents)

# Ø­Ù‚Ù„ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„
user_input = st.text_input("Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§:")

# Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
if user_input:
    with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©..."):
        response = chain({"question": user_input})
        st.write("Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:", response["answer"])
