import streamlit as st
import openai
import pdfplumber
import csv
import os
import json
from bs4 import BeautifulSoup
import requests
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.chains import ConversationalRetrievalChain
from langchain.schema import Document
from langchain.llms import OpenAI

# Ø¥Ø¹Ø¯Ø§Ø¯ Ù…ÙØªØ§Ø­ API Ø§Ù„Ø®Ø§Øµ Ø¨Ù€ OpenAI
openai.api_key = 'YOUR_OPENAI_API_KEY'  # Ø§Ø³ØªØ¨Ø¯Ù„ 'YOUR_OPENAI_API_KEY' Ø¨Ù…ÙØªØ§Ø­ API Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ

# Ø¯ÙˆØ§Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø«Ø§Ø¨ØªØ© (PDF, CSV, TXT, JSON, HTML)
def load_pdf(file_path):
    text = ""
    with pdfplumber.open(file_path) as pdf:
        for page in pdf.pages:
            text += page.extract_text()
    return text

def load_csv(file_path):
    text = ""
    with open(file_path, newline='', encoding="utf-8") as csvfile:
        reader = csv.reader(csvfile)
        for row in reader:
            text += " ".join(row) + "\n"
    return text

def load_txt(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        return file.read()

def load_json(file_path):
    text = ""
    with open(file_path, "r", encoding="utf-8") as file:
        data = json.load(file)
        text = json.dumps(data, ensure_ascii=False, indent=2)
    return text

def load_html(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        soup = BeautifulSoup(file, 'html.parser')
        paragraphs = soup.find_all('p')
        text = ' '.join([p.get_text() for p in paragraphs])
    return text

# ØªØ­Ù…ÙŠÙ„ Ù…Ø­ØªÙˆÙ‰ ØµÙØ­Ø© ÙˆÙŠØ¨ Ù…Ù† Ø±Ø§Ø¨Ø· HTML
def load_web_content(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    paragraphs = soup.find_all('p')
    text = ' '.join([p.get_text() for p in paragraphs])
    return text

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ù…Ù„Ù links.txt Ø£Ùˆ links.json ÙˆØªØ­Ù„ÙŠÙ„Ù‡Ø§ ÙƒØµÙØ­Ø§Øª HTML
def load_links():
    links_file_path = os.path.join("data", "links.json")
    documents = []

    if os.path.exists(links_file_path):
        try:
            with open(links_file_path, "r", encoding="utf-8") as file:
                data = json.load(file)
                for link in data["links"]:
                    content = load_web_content(link)
                    documents.append(Document(page_content=content, metadata={"source": link}))
        except json.JSONDecodeError:
            st.error("Ù‡Ù†Ø§Ùƒ Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø³ÙŠÙ‚ Ù…Ù„Ù links.json. ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ù„Ù ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ JSON ØµØ§Ù„Ø­.")
    
    return documents

# Ø¥Ø¹Ø¯Ø§Ø¯ LangChain
def setup_chain(documents):
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    docs = text_splitter.split_documents(documents)
    embeddings = OpenAIEmbeddings()
    db = FAISS.from_documents(docs, embeddings)
    return ConversationalRetrievalChain(OpenAI(), db.as_retriever())

# Ù‚Ø±Ø§Ø¡Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ù† Ù…Ø¬Ù„Ø¯ data ÙˆØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
def load_all_files():
    documents = []
    data_folder = "data"
    for filename in os.listdir(data_folder):
        file_path = os.path.join(data_folder, filename)
        content = None
        if filename.endswith(".pdf"):
            content = load_pdf(file_path)
        elif filename.endswith(".csv"):
            content = load_csv(file_path)
        elif filename.endswith(".txt") and filename != "links.txt":
            content = load_txt(file_path)
        elif filename.endswith(".json") and filename != "links.json":
            content = load_json(file_path)
        elif filename.endswith(".html") or filename.endswith(".htm"):
            content = load_html(file_path)
        
        if content:
            documents.append(Document(page_content=content, metadata={"source": filename}))

    # Ø¥Ø¶Ø§ÙØ© Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙƒØµÙØ­Ø§Øª HTML
    documents.extend(load_links())
    return documents

# ÙˆØ§Ø¬Ù‡Ø© Streamlit Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…
st.set_page_config(page_title="Chatbot - Ask Only", page_icon="ğŸ¤–")
st.title("Chatbot ğŸ¤– - Ø§Ø³Ø£Ù„Ù†ÙŠ ÙÙ‚Ø·")
st.write("ÙŠÙ…ÙƒÙ†Ùƒ Ø·Ø±Ø­ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø«Ø§Ø¨ØªØ© ÙˆØ§Ù„Ø±ÙˆØ§Ø¨Ø·.")

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª ÙˆØ§Ù„Ø±ÙˆØ§Ø¨Ø·
documents = load_all_files()
chain = setup_chain(documents)

# Ø­Ù‚Ù„ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„
user_input = st.text_input("Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§:")

# Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
if user_input:
    with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©..."):
        response = chain({"question": user_input})
        st.write("Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:", response["answer"])
